{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "회귀_1104_오전.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP2q0urUTCxQBO2MI13sUig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kibbm/ML-Tensorflow/blob/master/%ED%9A%8C%EA%B7%80_1104_%EC%98%A4%EC%A0%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ei10NVS5F68"
      },
      "source": [
        "참고> data모여 있는 곳...http://biostat.mc.vanderbilt.edu/wiki/Main/DataSets\n",
        "\n",
        "colab (화일업로드)경로설정>> https://leebaro.tistory.com/entry/google-Colaboratory-에서-구글-드라이브에-있는-파일-쉽이용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYn5edh61G4U"
      },
      "source": [
        "\n",
        "## **선형회귀(Linear Regression)**\n",
        "\n",
        "- https://velog.io/@amobmocmo/Python-단순-선형-회귀-Linear-Regression-구현-9ik2uej68q\n",
        "\n",
        "- https://blog.naver.com/PostView.nhn?blogId=tommybee&logNo=221755832342 (python, java, c++각 언어로 구현)\n",
        "- http://hleecaster.com/ml-linear-regression-example/ (코드 안됨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5u3XYt71GGE",
        "outputId": "fb122895-a7f6-4427-9fb2-8c5e14f4ce75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#샘플 값은 [1, 2, 3, 4, 5, 6] 타깃 값은 [9, 12, 15, 18, 21, 24]로 정의한다. 이 경우 학습이 완료됐을 때 W와 b는 각각 3, 6이 되어야 한다.\n",
        "x_train = np.array([1., 2., 3., 4., 5., 6.])\n",
        "y_train = np.array([9., 12., 15., 18., 21., 24.])\n",
        "\n",
        "# 초기의 가중치와 offset은 대부분 random 값으로 초기화 하지만 지금은 0으로 초기화한다.\n",
        "W = 0.0\n",
        "b = 0.0\n",
        "\n",
        "n_data = len(x_train)\n",
        "\n",
        "epochs = 5000\n",
        "learning_rate = 0.01\n",
        "\n",
        "# epoch은 전체 데이터 셋에 대해 한 번 학습을 하는 사이클을 의미한다. learning rate는 경사하강법을 사용할 때 파라미터 업데이트의 가중치를 의미하며 learning rate는 학습속도와 예측 정확도에 영향을 미치므로 적절히 설정해야 한다.\n",
        "for i in range(epochs):\n",
        "    hypothesis = x_train * W + b\n",
        "    cost = np.sum((hypothesis - y_train) ** 2) / n_data\n",
        "\n",
        "    # hypothesis : 예측 함수, cost : 목적(비용) 함수, 평균 제곱 오차\n",
        "    gradient_w = np.sum((W * x_train - y_train + b) * 2 * x_train) / n_data\n",
        "    gradient_b = np.sum((W * x_train - y_train + b) * 2) / n_data\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print('Epoch ({:10d}/{:10d}) cost: {:10f}, W: {:10f}, b:{:10f}'.format(i, epochs, cost, W, b))\n",
        "\n",
        "    W -= learning_rate * gradient_w\n",
        "    b -= learning_rate * gradient_b\n",
        "\n",
        "\n",
        "print('W: {:10f}'.format(W))\n",
        "print('b: {:10f}'.format(b))\n",
        "print('result : ')\n",
        "print(x_train * W + b)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch (         0/      5000) cost: 298.500000, W:   0.000000, b:  0.000000\n",
            "Epoch (       100/      5000) cost:   2.338782, W:   3.814322, b:  2.513728\n",
            "Epoch (       200/      5000) cost:   1.125703, W:   3.564954, b:  3.581321\n",
            "Epoch (       300/      5000) cost:   0.541823, W:   3.391949, b:  4.321987\n",
            "Epoch (       400/      5000) cost:   0.260790, W:   3.271924, b:  4.835841\n",
            "Epoch (       500/      5000) cost:   0.125524, W:   3.188653, b:  5.192339\n",
            "Epoch (       600/      5000) cost:   0.060417, W:   3.130882, b:  5.439667\n",
            "Epoch (       700/      5000) cost:   0.029080, W:   3.090803, b:  5.611256\n",
            "Epoch (       800/      5000) cost:   0.013997, W:   3.062996, b:  5.730300\n",
            "Epoch (       900/      5000) cost:   0.006737, W:   3.043705, b:  5.812890\n",
            "Epoch (      1000/      5000) cost:   0.003243, W:   3.030321, b:  5.870188\n",
            "Epoch (      1100/      5000) cost:   0.001561, W:   3.021036, b:  5.909940\n",
            "Epoch (      1200/      5000) cost:   0.000751, W:   3.014594, b:  5.937519\n",
            "Epoch (      1300/      5000) cost:   0.000362, W:   3.010125, b:  5.956652\n",
            "Epoch (      1400/      5000) cost:   0.000174, W:   3.007025, b:  5.969927\n",
            "Epoch (      1500/      5000) cost:   0.000084, W:   3.004873, b:  5.979136\n",
            "Epoch (      1600/      5000) cost:   0.000040, W:   3.003381, b:  5.985525\n",
            "Epoch (      1700/      5000) cost:   0.000019, W:   3.002346, b:  5.989958\n",
            "Epoch (      1800/      5000) cost:   0.000009, W:   3.001627, b:  5.993033\n",
            "Epoch (      1900/      5000) cost:   0.000004, W:   3.001129, b:  5.995166\n",
            "Epoch (      2000/      5000) cost:   0.000002, W:   3.000783, b:  5.996647\n",
            "Epoch (      2100/      5000) cost:   0.000001, W:   3.000543, b:  5.997674\n",
            "Epoch (      2200/      5000) cost:   0.000001, W:   3.000377, b:  5.998386\n",
            "Epoch (      2300/      5000) cost:   0.000000, W:   3.000262, b:  5.998880\n",
            "Epoch (      2400/      5000) cost:   0.000000, W:   3.000181, b:  5.999223\n",
            "Epoch (      2500/      5000) cost:   0.000000, W:   3.000126, b:  5.999461\n",
            "Epoch (      2600/      5000) cost:   0.000000, W:   3.000087, b:  5.999626\n",
            "Epoch (      2700/      5000) cost:   0.000000, W:   3.000061, b:  5.999741\n",
            "Epoch (      2800/      5000) cost:   0.000000, W:   3.000042, b:  5.999820\n",
            "Epoch (      2900/      5000) cost:   0.000000, W:   3.000029, b:  5.999875\n",
            "Epoch (      3000/      5000) cost:   0.000000, W:   3.000020, b:  5.999913\n",
            "Epoch (      3100/      5000) cost:   0.000000, W:   3.000014, b:  5.999940\n",
            "Epoch (      3200/      5000) cost:   0.000000, W:   3.000010, b:  5.999958\n",
            "Epoch (      3300/      5000) cost:   0.000000, W:   3.000007, b:  5.999971\n",
            "Epoch (      3400/      5000) cost:   0.000000, W:   3.000005, b:  5.999980\n",
            "Epoch (      3500/      5000) cost:   0.000000, W:   3.000003, b:  5.999986\n",
            "Epoch (      3600/      5000) cost:   0.000000, W:   3.000002, b:  5.999990\n",
            "Epoch (      3700/      5000) cost:   0.000000, W:   3.000002, b:  5.999993\n",
            "Epoch (      3800/      5000) cost:   0.000000, W:   3.000001, b:  5.999995\n",
            "Epoch (      3900/      5000) cost:   0.000000, W:   3.000001, b:  5.999997\n",
            "Epoch (      4000/      5000) cost:   0.000000, W:   3.000001, b:  5.999998\n",
            "Epoch (      4100/      5000) cost:   0.000000, W:   3.000000, b:  5.999998\n",
            "Epoch (      4200/      5000) cost:   0.000000, W:   3.000000, b:  5.999999\n",
            "Epoch (      4300/      5000) cost:   0.000000, W:   3.000000, b:  5.999999\n",
            "Epoch (      4400/      5000) cost:   0.000000, W:   3.000000, b:  5.999999\n",
            "Epoch (      4500/      5000) cost:   0.000000, W:   3.000000, b:  6.000000\n",
            "Epoch (      4600/      5000) cost:   0.000000, W:   3.000000, b:  6.000000\n",
            "Epoch (      4700/      5000) cost:   0.000000, W:   3.000000, b:  6.000000\n",
            "Epoch (      4800/      5000) cost:   0.000000, W:   3.000000, b:  6.000000\n",
            "Epoch (      4900/      5000) cost:   0.000000, W:   3.000000, b:  6.000000\n",
            "W:   3.000000\n",
            "b:   6.000000\n",
            "result : \n",
            "[ 8.99999996 11.99999997 14.99999998 18.         21.00000001 24.00000002]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojn05izB2EI3"
      },
      "source": [
        "gradient_w는 목적 함수인 평균 제곱 오차를 W에 대해 편미분해 계산한 값이며 경사하강법을 적용해 다음 W를 학습률과 곱해 업데이트한다.\n",
        "마찬가지로 gradient_b는 b에 대한 편미분 계산값이다.\n",
        "\n",
        "\n",
        "~> 학습을 진행하며 W와 b가 각각 3, 6에 가까워짐을 볼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfwTGtjb3jD_"
      },
      "source": [
        "\n",
        "\n",
        "# **로지스틱회귀(Logistic Regression)**..타이타닉\n",
        "http://hleecaster.com/ml-logistic-regression-example/ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX5keacN51nH",
        "outputId": "03f4d978-ee34-4ac7-cd8e-93da6e6b032e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSBu92oy802S",
        "outputId": "9c77f156-e695-47eb-a4c7-1233dca47767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "#구글 클라우드 경로 test\n",
        "\n",
        "with open('/content/gdrive/My Drive/foo.txt', 'w') as f:\n",
        "  f.write('Hello Google Drive!')\n",
        "!cat /content/gdrive/My\\ Drive/foo.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7b4d55f63e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#구글 클라우드 경로 test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/foo.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hello Google Drive!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat /content/gdrive/My\\\\ Drive/foo.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/foo.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aMEhC-k3xo6",
        "outputId": "e3681ecb-f39a-41e5-e270-7185f0ba0c1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 데이터 다운로드 https://public.opendatasoft.com/explore/dataset/titanic-passengers/export/\n",
        "# 코드 참조 http://hleecaster.com/ml-logistic-regression-example/\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#1. 데이터 (csv)불러오기\n",
        "#passengers = pd.read_csv(\"C:/Users/kibbm/Downloads/titanic-passengers.csv\", encoding='utf-8')\n",
        "#passengers = pd.read_csv(\"C:/Users/kibbm/Downloads/titanic3.csv\", encoding='utf-8')\n",
        "passengers = pd.read_csv(\"/tmp/titanic3.csv\")\n",
        "\n",
        "\n",
        "print(passengers.shape) #891명의 데이터가 있고, 총 12개의 컬럼\n",
        "print(passengers.head()) #데이터의 상위 5개 불러오기\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1309, 14)\n",
            "   pclass  survived  ...   body                        home.dest\n",
            "0       1         1  ...    NaN                     St Louis, MO\n",
            "1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON\n",
            "2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
            "3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON\n",
            "4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
            "\n",
            "[5 rows x 14 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8prWFL0UGmoY",
        "outputId": "eeb1eac6-39af-4d9b-e00a-e63b85922a56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "#2. 데이터 전처리하기\n",
        "#2.1 분석에 사용할 feature고르기\n",
        "passengers['sex'] = passengers['sex'].map({'female':1, 'male':0})\n",
        "passengers['age'].fillna(value=passengers['age'].mean(), inplace=True)\n",
        "\n",
        "#2.2 feature분리하기\n",
        "passengers['FirstClass'] = passengers['pclass'].apply(lambda x:1 if x== 1 else 0)\n",
        "passengers['SecondClass'] = passengers['pclass'].apply(lambda x:1 if x==2 else 0)\n",
        "\n",
        "features = passengers[['sex', 'age', 'FirstClass', 'SecondClass']]\n",
        "survival = passengers['survived']\n",
        "\n",
        "#3. 학습세트/평가세트 분리하기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(features, survival)\n",
        "\n",
        "#4. 데이터 정규화(스케일링) 하기\n",
        "# - sklearn이 제공하는 StandardScaler를 활용해서 손쉽게 할 수 있다. StandardScaler는 평균 0, 표준편차 1로 변환하는 방법이지만 이외에도 최소값 0, 최대값 1이 되도록 변환하는 MinMaxScaler, 중앙값(median) 0, IQR(interquartile range) 1이 되도록 변환하는 RobustScaler 등이 있다.\n",
        "# - fit_transform 은 fit과 transform을 합친 건데, fit은 일단 각 속성(feature)마다 컬럼을 만드는 작업이라고 생각하면 된다. 이후 transform을 통해 데이터를 변형시키는 거다.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_feartures = scaler.fit_transform(train_features)\n",
        "test_features = scaler.transform(test_features)\n",
        "\n",
        "#5. 모델 생성 및 평가하기\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(train_features, train_labels)\n",
        "\n",
        "print()#줄바꿈\n",
        "print(model.score(train_features, train_labels))\n",
        "\n",
        "    #아까 분리해놓은 테스트 세트에서도 확인해볼 수 있다.\n",
        "print(model.score(test_features, test_labels))\n",
        "\n",
        "    # 각 feature들의 계수(coefficients)를 확인해볼 차례다. 어떤 feature가 생존에 큰 영향을 주는지 확인해볼 수 있으니까.\n",
        "    #Sex, Age, FirstClass, SecondClass 순으로 넣었기 때문에 그 순서대로 확인..성별이 1(여자)이고, 일등석 탑승 여부가 중요하다는 걸 알 수 있다. 반면 나이에 대한 계수는 음수가 나오는데 나이가 많을수록 생존 확률이 낮아진다는 의미로 이해하면 되겠다.\n",
        "print(model.coef_)\n",
        "\n",
        "#6. 예측하기\n",
        "    #이번엔 새로운 데이터를 넣어서 예측해보자...예를 들면 아래와 같이 타이타닉 영화 실제 주인공이었던 Jack, Rose의 값을 임의로 만들고, 내 정보도 넣어봤다. (가상이니까… 1등석에 탄 걸로 해봤다.)\n",
        "import numpy as np\n",
        "\n",
        "Jack = np.array([0.0, 20.0, 0.0, 0.0])\n",
        "Rose = np.array([1.0, 17.0, 1.0, 0.0])\n",
        "ME = np.array([1.0, 32.0, 1.0, 0.0])\n",
        "\n",
        "\n",
        "sample_passengers = np.array([Jack, Rose, ME])\n",
        "#sample_passengers = np.array([Jack, Rose])\n",
        "\n",
        "sample_passengers = scaler.transform(sample_passengers) \n",
        "print(model.predict(sample_passengers))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0.781855249745158\n",
            "0.7865853658536586\n",
            "[[ 2.40838194 -0.02641165  1.92397421  0.81923197]]\n",
            "[0 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF-NaV51Cbif"
      },
      "source": [
        "#**유방암 예측 예시\n",
        " - https://myjamong.tistory.com/80 **"
      ]
    }
  ]
}